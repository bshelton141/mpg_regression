---
title: "Miles per Gallon Regression Analysis"
author: "Brandon Shelton"
date: "July 19, 2016"
output: html_document
---

##Overview

The goal of this analysis is to create a reliable regression model for predicting a car's miles per gallon (`mpg`), using the `datasets` package's `mtcars` data set. This document will prove that over 80% of the variation in the data set's `mpg` can be accounted for through a normal linear regression that uses only `wt` and `hp` as predictors. It will also show several model analysis techniques to ensure that no additional processing is necessary.

The visualizations generated by the code in this document can be found at my rpubs account [here](http://rpubs.com/bshelton141/198354).

Begin by loading the packages and data set that will be used:

```{r}
library(ggplot2); library(gridExtra); library(Metrics); library(car)
data(mtcars)
```

##Exploratory Analysis

There are only 32 rows in the data set, and each row contains 11 automobile specifications for car models from 1973-74. Upfront, there is a concern that the lack of data points may produce a regression model that has a very large bias to the cars that it includes. i.e., the model may not be good at predicting `mpg` for cars outside of this data set.

```{r}
dim(mtcars)
head(mtcars)
```


A histogram of `mpg` proves that the small data set is relatively Gaussian. Also, because there is not an unbound upper-limit to `mpg`, nor is it categorical, it makes the most sense to apply a general linear regression model to `mtcars`.

```{r}
ggplot(data = mtcars, aes(x = mpg)) + geom_histogram(binwidth = 5)
```

The regression analysis will begin with a model that includes all possible predictors, since there are not that many of them to digest. However, there are a few that stand out as potentially being significant to predicting `mpg`: `hp`, `wt`, and `am` (the latter being binomial).

```{r}
vshp <- ggplot(data = mtcars, aes(x = hp, y = mpg)) + 
  geom_point(colour = "forestgreen", size = 3) + 
  labs(title = "MPG vs. HP") + 
  geom_smooth(method='lm')
vswt <- ggplot(data = mtcars, aes(x = wt, y = mpg)) + 
  geom_point(colour = "blue", size = 3) + 
  labs(title = "MPG vs. WT") + 
  geom_smooth(method='lm')
vsam <- ggplot(data = mtcars, aes(x = as.factor(am), y = mpg, fill = as.factor(am))) + 
  geom_boxplot(color = "blue") + 
  labs(title = "MPG vs. AM") + 
  geom_jitter(position=position_jitter(width=.5), colour = "purple", size = 3) +
  theme(legend.position="none")
  
vshp
vswt
vsam
```


##Regression Model Building

We will build and evaluate several regression models for predicting `mpg`. The first one we will look at is a linear regression that includes all 9 predictor variables. Our p-value significance level for this analysis is *p <= .05*, which means that there is only a 5% chance that the variable is mistakenly significant to the model. Using the `summary` function, it is evident that this combination of predictors does not include any that meet our p-value significance level. 

```{r}
test1 <- lm(mpg ~ ., data = mtcars)
summary(test1)
```

Next, we will review a linear regression model that only includes the three predictors that are illustrated above: `hp`, `wt`, and `am`. This model contains two predictors with appropriately significant p-values (`hp` and `wt`) and one that is still not significant enough (`am`) because its p-value is 14%. **Because the `am` p-value is above our threshold, `am` must be removed from the model.**

```{r}
test2 <- lm(mpg ~ hp + wt + am, data = mtcars)
summary(test2)
```

The final model we will create is a linear regression model that includes only `hp` and `wt`as predictors. The summary of this model proves that both predictors are statistically significant to the model and that they provide an Adjusted R-squared value of 81.5%, meaning that over 80% of the `mpg` variance can be accounted for by `hp` and `wt`. **The model indicates that `mpg` increases as both `wt` and `hp` descrease.**

```{r}
test3 <- lm(mpg ~ wt + hp, data = mtcars)
summary(test3)
```

Before we accept this model, we want to validate that `wt` and `hp` are not over-inflating each others' regression coefficients due correlation. This can be assessed by reviewing their Variance Inflation Factors (VIF). The VIF for both predictors is well below 2.5, meaning that they are not significantly correlated.

```{r}
vif(test3)
```

Lastly, the residuals in the plot below seem to be distributed randomly and evenly, which is what we hoped to see. If there were a noticeable pattern or the variance around zero increased/decreased as `mpg` increased (i.e., heteroscedasticity), then we would need to revisit the model to understand what would be driving the pattern, and account for it.

```{r}
ggplot(data = test3, aes(x = mpg, y = resid(test1))) + 
  geom_point(size = 2) + 
  geom_hline(yintercept = 0, colour = "red") +
  labs(x = "MPG", y = "Residuals", title = "Residual Plot")
```


